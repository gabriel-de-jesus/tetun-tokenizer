from typing import List, Callable
from pathlib import Path


class Evaluation:
    """ 
    A class used for evaluating the accuracy of a tokenizer. It provides functionality 
    for visualizing the unmatched tokens. 
    """

    def __init__(self, input_text_file: Path, human_tokens_file: Path, tokenizer: Callable) -> None:
        self.input_text_file = input_text_file
        self.human_tokens_file = human_tokens_file
        self.tokenizer = tokenizer

    def load_data(self, file_path: Path, human_input=True) -> List[str]:
        """
        Load data from:
        1. Human tokens (ground truth), where it contains each tokens per line.
        2. Input file to be tokenize by the tokenizer (human_input=False).

        :param file_path: a path of the input file to read.
        :param human_input: True if it reads the human_tokens file, False otherwise.
        :return: a list of string.
        """
        try:
            with file_path.open('r', encoding='utf-8') as f_load:
                if human_input:
                    contents = [line.strip() for line in f_load.readlines()]
                else:
                    contents = " ".join(f_load.readlines())
        except FileNotFoundError:
            print(f"File not found at: {file_path}")
            return []
        except UnicodeEncodeError:
            print(f"Can not decode file at: {file_path}")
            return []
        return contents

    def tokenizer_tokens(self) -> List[str]:
        """ Read and tokenize the input file and return a list of string. """
        input_text = self.load_data(self.input_text_file, human_input=False)
        tokens = self.tokenizer.tokenize(input_text)
        # Since TetunBlankLineTokenizer() returns a single list string, we need to create a new list splitting by '\n'.
        if "TetunBlankLineTokenizer" in type(self.tokenizer).__name__:
            tokens = [token.strip() for token in tokens[0].split('\n')]
        return tokens

    def human_tokens(self) -> List[str]:
        """ Read the file contained human tokens (ground truth) data and return a list of string. """
        tokens = self.load_data(self.human_tokens_file)
        return tokens

    def get_total_elements(self) -> int:
        """ Compare the lists length of the tokenizer and humans. If they are different, get the minimum one. Then, return it. """
        if len(self.tokenizer_tokens()) != len(self.human_tokens()):
            total_elements = min(len(self.tokenizer_tokens()),
                                 len(self.human_tokens()))
        else:
            total_elements = len(self.tokenizer_tokens())
        return total_elements

    def calcualte_accuracy(self) -> str:
        """ Calculate the accuracy of tokens generated by the tokenizer and return it. """
        matching_elements = sum(1 for i in range(
            self.get_total_elements()) if self.tokenizer_tokens()[i] == self.human_tokens()[i])
        accuracy = (matching_elements / self.get_total_elements()) * 100
        accuracy = print(f"The tokenizer accuracy is: {accuracy:.2f}%.")
        return accuracy

    def visualize_unmatched(self) -> None:
        """ Visualize unmatch tokens. """
        unmatched_tokens = []
        for i in range(self.get_total_elements()):
            if self.tokenizer_tokens()[i] != self.human_tokens()[i]:
                unmatched_tokens.append(
                    (self.tokenizer_tokens()[i], self.human_tokens()[i]))

        if len(unmatched_tokens) > 0:
            print(f"\nThe following tokens are not matched: ")
            for token in unmatched_tokens:
                print(
                    f"\tToken '{token[0]}' in tokenizer_tokens and '{token[1]}' in human_tokens.")

    def evaluate(self) -> None:
        """ Calcualte accuracy and visualize unmatched tokens. """
        print("\n============================")
        if self.tokenizer_tokens() != self.human_tokens():
            print(
                "The token lists generated by the tokenizer and the human input are of different lengths!")

        self.calcualte_accuracy()
        self.visualize_unmatched()
        print("============================\n")
